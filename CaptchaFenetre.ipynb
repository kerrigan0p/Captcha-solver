{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "TEBgRGBSAaeW",
        "outputId": "19bcdec8-4fbf-4374-cd69-a04f06f875d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground truth coordinates: x1=105, y1=175, x2=180, y2=207\n",
            "Ground truth coordinates: x1=40, y1=75, x2=115, y2=107\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def draw_labels_on_image(image_path, labels_file):\n",
        "    \"\"\"\n",
        "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
        "    and displays the image with red circles at those coordinates.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image.\n",
        "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Unable to load image: {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
        "    image_resized = image#cv2.resize(image, (340, 410))\n",
        "\n",
        "    # Read labels from the file\n",
        "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
        "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
        "\n",
        "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
        "    if 'img_name' in labels_df.columns:\n",
        "        row = labels_df[labels_df['img_name'] == image_name]\n",
        "    else:\n",
        "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
        "\n",
        "    if row.empty:\n",
        "        print(f\"No labels found for {image_name}\")\n",
        "        return\n",
        "\n",
        "    # Extract ground truth coordinates\n",
        "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
        "    x1 = int(x1)\n",
        "    x2 = int(x2)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "    # Draw red circles at the ground truth coordinates\n",
        "    image_drawn = image_resized.copy()\n",
        "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
        "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
        "\n",
        "    # Display the image with the drawn points\n",
        "    cv2.imshow(\"img\",image_drawn)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "extract_dir = 'extracted_captchas'\n",
        "processed_images_dir = \"processed_captchas\"\n",
        "image_path = f\"{extract_dir}/captchas_saved/captcha_8.png\"\n",
        "labels_file = \"labels.txt\"\n",
        "draw_labels_on_image(image_path,labels_file)\n",
        "image_path = f\"truncated_captchas/captcha_8.png\"\n",
        "labels_file = \"truncated_labels.csv\"\n",
        "draw_labels_on_image(image_path,labels_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "Pamd0yPYBJhm",
        "outputId": "3901a604-edf0-421b-f359-220a765b6f85"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def show_pixels(image_path,nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite):\n",
        "    \"\"\"\n",
        "    Displays the top 20 rows of an image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image was loaded correctly\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    truncated_image = image[nbPixelsBas:nbPixelsHaut, nbPixelsGauche:nbPixelsDroite]\n",
        "\n",
        "    # Display the cropped section\n",
        "    cv2.imshow(\"img\",truncated_image)  # Use cv2_imshow in Google Colab, replace with cv2.imshow() for local use\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "extract_dir = 'extracted_captchas'\n",
        "processed_images_dir = \"processed_captchas\"\n",
        "image_path = f\"{extract_dir}/captchas_saved/captcha_315.png\"\n",
        "labels_file = \"labels.txt\"\n",
        "\n",
        "nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 310,100,65,275 # Images tronquées\n",
        "# nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 55,5,185,235 # Premier perso\n",
        "# nbPixelsHaut, nbPixelsBas, nbPixelsGauche, nbPixelsDroite = 50,10, 265, 305 #Second perso\n",
        "show_pixels(image_path,nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkXTRcDNEJnf",
        "outputId": "2d998411-6424-4b43-b5d6-58ca188128b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['img_name', 'x1', 'y1', 'x2', 'y2'], dtype='object')\n",
            "            img_name          x1          y1          x2          y2\n",
            "0      captcha_1.png   46.816017  146.209957   54.581169   18.639610\n",
            "1     captcha_10.png   54.581169  143.991342  155.528139   67.449134\n",
            "2    captcha_100.png  103.390693  192.800866   22.411255  191.691558\n",
            "3    captcha_101.png  142.216450  125.133117   60.127706  165.068182\n",
            "4    captcha_102.png   95.625541  130.679654  155.528139  170.614719\n",
            "..               ...         ...         ...         ...         ...\n",
            "311   captcha_95.png   83.423160   35.279221  122.248918  182.817100\n",
            "312   captcha_96.png   49.034632   40.825758   44.597403  172.833333\n",
            "313   captcha_97.png   45.706710  170.614719  138.888528   92.963203\n",
            "314   captcha_98.png   45.706710  105.165584  176.604978   49.700216\n",
            "315   captcha_99.png   42.378788  187.254329   23.520563  117.367965\n",
            "\n",
            "[316 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def adjust_x_coordinates(label, nbPixelsGauche):\n",
        "    return label - nbPixelsGauche\n",
        "\n",
        "def adjust_y_coordinates(label, nbPixelsBas):\n",
        "    return label - nbPixelsBas\n",
        "\n",
        "# Load the labels file\n",
        "df = pd.read_csv(\"labels.txt\")\n",
        "print(df.columns)\n",
        "# Ensure the coordinate columns are numeric\n",
        "df[['x1', 'y1', 'x2', 'y2']] = df[['x1', 'y1', 'x2', 'y2']].apply(pd.to_numeric)\n",
        "\n",
        "nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 310,100,65,275 # Images tronquées\n",
        "# Adjust coordinates\n",
        "df['x1'] = df['x1'].apply(lambda x: adjust_x_coordinates(x, nbPixelsGauche))\n",
        "df['x2'] = df['x2'].apply(lambda x: adjust_x_coordinates(x, nbPixelsGauche))\n",
        "df['y1'] = df['y1'].apply(lambda y: adjust_y_coordinates(y, nbPixelsBas))\n",
        "df['y2'] = df['y2'].apply(lambda y: adjust_y_coordinates(y, nbPixelsBas))\n",
        "\n",
        "# Save the adjusted labels\n",
        "df.to_csv('truncated_labels.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZZ0f7xyI9YM",
        "outputId": "d30f2318-a07a-4fd4-b7ad-b683490af353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved cropped image: truncated_captchas\\captcha_1.png\n",
            "Saved cropped image: truncated_captchas\\captcha_10.png\n",
            "Saved cropped image: truncated_captchas\\captcha_100.png\n",
            "Saved cropped image: truncated_captchas\\captcha_101.png\n",
            "Saved cropped image: truncated_captchas\\captcha_102.png\n",
            "Saved cropped image: truncated_captchas\\captcha_103.png\n",
            "Saved cropped image: truncated_captchas\\captcha_104.png\n",
            "Saved cropped image: truncated_captchas\\captcha_105.png\n",
            "Saved cropped image: truncated_captchas\\captcha_106.png\n",
            "Saved cropped image: truncated_captchas\\captcha_107.png\n",
            "Saved cropped image: truncated_captchas\\captcha_108.png\n",
            "Saved cropped image: truncated_captchas\\captcha_109.png\n",
            "Saved cropped image: truncated_captchas\\captcha_11.png\n",
            "Saved cropped image: truncated_captchas\\captcha_110.png\n",
            "Saved cropped image: truncated_captchas\\captcha_111.png\n",
            "Saved cropped image: truncated_captchas\\captcha_112.png\n",
            "Saved cropped image: truncated_captchas\\captcha_113.png\n",
            "Saved cropped image: truncated_captchas\\captcha_114.png\n",
            "Saved cropped image: truncated_captchas\\captcha_115.png\n",
            "Saved cropped image: truncated_captchas\\captcha_116.png\n",
            "Saved cropped image: truncated_captchas\\captcha_117.png\n",
            "Saved cropped image: truncated_captchas\\captcha_118.png\n",
            "Saved cropped image: truncated_captchas\\captcha_119.png\n",
            "Saved cropped image: truncated_captchas\\captcha_12.png\n",
            "Saved cropped image: truncated_captchas\\captcha_120.png\n",
            "Saved cropped image: truncated_captchas\\captcha_121.png\n",
            "Saved cropped image: truncated_captchas\\captcha_122.png\n",
            "Saved cropped image: truncated_captchas\\captcha_123.png\n",
            "Saved cropped image: truncated_captchas\\captcha_124.png\n",
            "Saved cropped image: truncated_captchas\\captcha_125.png\n",
            "Saved cropped image: truncated_captchas\\captcha_126.png\n",
            "Saved cropped image: truncated_captchas\\captcha_127.png\n",
            "Saved cropped image: truncated_captchas\\captcha_128.png\n",
            "Saved cropped image: truncated_captchas\\captcha_129.png\n",
            "Saved cropped image: truncated_captchas\\captcha_13.png\n",
            "Saved cropped image: truncated_captchas\\captcha_130.png\n",
            "Saved cropped image: truncated_captchas\\captcha_131.png\n",
            "Saved cropped image: truncated_captchas\\captcha_132.png\n",
            "Saved cropped image: truncated_captchas\\captcha_133.png\n",
            "Saved cropped image: truncated_captchas\\captcha_134.png\n",
            "Saved cropped image: truncated_captchas\\captcha_135.png\n",
            "Saved cropped image: truncated_captchas\\captcha_136.png\n",
            "Saved cropped image: truncated_captchas\\captcha_137.png\n",
            "Saved cropped image: truncated_captchas\\captcha_138.png\n",
            "Saved cropped image: truncated_captchas\\captcha_139.png\n",
            "Saved cropped image: truncated_captchas\\captcha_14.png\n",
            "Saved cropped image: truncated_captchas\\captcha_140.png\n",
            "Saved cropped image: truncated_captchas\\captcha_141.png\n",
            "Saved cropped image: truncated_captchas\\captcha_142.png\n",
            "Saved cropped image: truncated_captchas\\captcha_143.png\n",
            "Saved cropped image: truncated_captchas\\captcha_144.png\n",
            "Saved cropped image: truncated_captchas\\captcha_145.png\n",
            "Saved cropped image: truncated_captchas\\captcha_146.png\n",
            "Saved cropped image: truncated_captchas\\captcha_147.png\n",
            "Saved cropped image: truncated_captchas\\captcha_148.png\n",
            "Saved cropped image: truncated_captchas\\captcha_149.png\n",
            "Saved cropped image: truncated_captchas\\captcha_15.png\n",
            "Saved cropped image: truncated_captchas\\captcha_150.png\n",
            "Saved cropped image: truncated_captchas\\captcha_151.png\n",
            "Saved cropped image: truncated_captchas\\captcha_152.png\n",
            "Saved cropped image: truncated_captchas\\captcha_153.png\n",
            "Saved cropped image: truncated_captchas\\captcha_154.png\n",
            "Saved cropped image: truncated_captchas\\captcha_155.png\n",
            "Saved cropped image: truncated_captchas\\captcha_156.png\n",
            "Saved cropped image: truncated_captchas\\captcha_157.png\n",
            "Saved cropped image: truncated_captchas\\captcha_158.png\n",
            "Saved cropped image: truncated_captchas\\captcha_159.png\n",
            "Saved cropped image: truncated_captchas\\captcha_16.png\n",
            "Saved cropped image: truncated_captchas\\captcha_160.png\n",
            "Saved cropped image: truncated_captchas\\captcha_161.png\n",
            "Saved cropped image: truncated_captchas\\captcha_162.png\n",
            "Saved cropped image: truncated_captchas\\captcha_163.png\n",
            "Saved cropped image: truncated_captchas\\captcha_164.png\n",
            "Saved cropped image: truncated_captchas\\captcha_165.png\n",
            "Saved cropped image: truncated_captchas\\captcha_166.png\n",
            "Saved cropped image: truncated_captchas\\captcha_167.png\n",
            "Saved cropped image: truncated_captchas\\captcha_168.png\n",
            "Saved cropped image: truncated_captchas\\captcha_169.png\n",
            "Saved cropped image: truncated_captchas\\captcha_17.png\n",
            "Saved cropped image: truncated_captchas\\captcha_170.png\n",
            "Saved cropped image: truncated_captchas\\captcha_171.png\n",
            "Saved cropped image: truncated_captchas\\captcha_172.png\n",
            "Saved cropped image: truncated_captchas\\captcha_173.png\n",
            "Saved cropped image: truncated_captchas\\captcha_174.png\n",
            "Saved cropped image: truncated_captchas\\captcha_175.png\n",
            "Saved cropped image: truncated_captchas\\captcha_176.png\n",
            "Saved cropped image: truncated_captchas\\captcha_177.png\n",
            "Saved cropped image: truncated_captchas\\captcha_178.png\n",
            "Saved cropped image: truncated_captchas\\captcha_179.png\n",
            "Saved cropped image: truncated_captchas\\captcha_18.png\n",
            "Saved cropped image: truncated_captchas\\captcha_180.png\n",
            "Saved cropped image: truncated_captchas\\captcha_181.png\n",
            "Saved cropped image: truncated_captchas\\captcha_182.png\n",
            "Saved cropped image: truncated_captchas\\captcha_183.png\n",
            "Saved cropped image: truncated_captchas\\captcha_184.png\n",
            "Saved cropped image: truncated_captchas\\captcha_185.png\n",
            "Saved cropped image: truncated_captchas\\captcha_186.png\n",
            "Saved cropped image: truncated_captchas\\captcha_187.png\n",
            "Saved cropped image: truncated_captchas\\captcha_188.png\n",
            "Saved cropped image: truncated_captchas\\captcha_189.png\n",
            "Saved cropped image: truncated_captchas\\captcha_19.png\n",
            "Saved cropped image: truncated_captchas\\captcha_190.png\n",
            "Saved cropped image: truncated_captchas\\captcha_191.png\n",
            "Saved cropped image: truncated_captchas\\captcha_192.png\n",
            "Saved cropped image: truncated_captchas\\captcha_193.png\n",
            "Saved cropped image: truncated_captchas\\captcha_194.png\n",
            "Saved cropped image: truncated_captchas\\captcha_195.png\n",
            "Saved cropped image: truncated_captchas\\captcha_196.png\n",
            "Saved cropped image: truncated_captchas\\captcha_197.png\n",
            "Saved cropped image: truncated_captchas\\captcha_198.png\n",
            "Saved cropped image: truncated_captchas\\captcha_199.png\n",
            "Saved cropped image: truncated_captchas\\captcha_2.png\n",
            "Saved cropped image: truncated_captchas\\captcha_20.png\n",
            "Saved cropped image: truncated_captchas\\captcha_200.png\n",
            "Saved cropped image: truncated_captchas\\captcha_201.png\n",
            "Saved cropped image: truncated_captchas\\captcha_202.png\n",
            "Saved cropped image: truncated_captchas\\captcha_203.png\n",
            "Saved cropped image: truncated_captchas\\captcha_204.png\n",
            "Saved cropped image: truncated_captchas\\captcha_205.png\n",
            "Saved cropped image: truncated_captchas\\captcha_206.png\n",
            "Saved cropped image: truncated_captchas\\captcha_207.png\n",
            "Saved cropped image: truncated_captchas\\captcha_208.png\n",
            "Saved cropped image: truncated_captchas\\captcha_209.png\n",
            "Saved cropped image: truncated_captchas\\captcha_21.png\n",
            "Saved cropped image: truncated_captchas\\captcha_210.png\n",
            "Saved cropped image: truncated_captchas\\captcha_211.png\n",
            "Saved cropped image: truncated_captchas\\captcha_212.png\n",
            "Saved cropped image: truncated_captchas\\captcha_213.png\n",
            "Saved cropped image: truncated_captchas\\captcha_214.png\n",
            "Saved cropped image: truncated_captchas\\captcha_215.png\n",
            "Saved cropped image: truncated_captchas\\captcha_216.png\n",
            "Saved cropped image: truncated_captchas\\captcha_217.png\n",
            "Saved cropped image: truncated_captchas\\captcha_218.png\n",
            "Saved cropped image: truncated_captchas\\captcha_219.png\n",
            "Saved cropped image: truncated_captchas\\captcha_22.png\n",
            "Saved cropped image: truncated_captchas\\captcha_220.png\n",
            "Saved cropped image: truncated_captchas\\captcha_221.png\n",
            "Saved cropped image: truncated_captchas\\captcha_222.png\n",
            "Saved cropped image: truncated_captchas\\captcha_223.png\n",
            "Saved cropped image: truncated_captchas\\captcha_224.png\n",
            "Saved cropped image: truncated_captchas\\captcha_225.png\n",
            "Saved cropped image: truncated_captchas\\captcha_226.png\n",
            "Saved cropped image: truncated_captchas\\captcha_227.png\n",
            "Saved cropped image: truncated_captchas\\captcha_228.png\n",
            "Saved cropped image: truncated_captchas\\captcha_229.png\n",
            "Saved cropped image: truncated_captchas\\captcha_23.png\n",
            "Saved cropped image: truncated_captchas\\captcha_230.png\n",
            "Saved cropped image: truncated_captchas\\captcha_231.png\n",
            "Saved cropped image: truncated_captchas\\captcha_232.png\n",
            "Saved cropped image: truncated_captchas\\captcha_233.png\n",
            "Saved cropped image: truncated_captchas\\captcha_234.png\n",
            "Saved cropped image: truncated_captchas\\captcha_235.png\n",
            "Saved cropped image: truncated_captchas\\captcha_236.png\n",
            "Saved cropped image: truncated_captchas\\captcha_237.png\n",
            "Saved cropped image: truncated_captchas\\captcha_238.png\n",
            "Saved cropped image: truncated_captchas\\captcha_239.png\n",
            "Saved cropped image: truncated_captchas\\captcha_24.png\n",
            "Saved cropped image: truncated_captchas\\captcha_240.png\n",
            "Saved cropped image: truncated_captchas\\captcha_241.png\n",
            "Saved cropped image: truncated_captchas\\captcha_242.png\n",
            "Saved cropped image: truncated_captchas\\captcha_243.png\n",
            "Saved cropped image: truncated_captchas\\captcha_244.png\n",
            "Saved cropped image: truncated_captchas\\captcha_245.png\n",
            "Saved cropped image: truncated_captchas\\captcha_246.png\n",
            "Saved cropped image: truncated_captchas\\captcha_247.png\n",
            "Saved cropped image: truncated_captchas\\captcha_248.png\n",
            "Saved cropped image: truncated_captchas\\captcha_249.png\n",
            "Saved cropped image: truncated_captchas\\captcha_25.png\n",
            "Saved cropped image: truncated_captchas\\captcha_250.png\n",
            "Saved cropped image: truncated_captchas\\captcha_251.png\n",
            "Saved cropped image: truncated_captchas\\captcha_252.png\n",
            "Saved cropped image: truncated_captchas\\captcha_253.png\n",
            "Saved cropped image: truncated_captchas\\captcha_254.png\n",
            "Saved cropped image: truncated_captchas\\captcha_255.png\n",
            "Saved cropped image: truncated_captchas\\captcha_256.png\n",
            "Saved cropped image: truncated_captchas\\captcha_257.png\n",
            "Saved cropped image: truncated_captchas\\captcha_258.png\n",
            "Saved cropped image: truncated_captchas\\captcha_259.png\n",
            "Saved cropped image: truncated_captchas\\captcha_26.png\n",
            "Saved cropped image: truncated_captchas\\captcha_260.png\n",
            "Saved cropped image: truncated_captchas\\captcha_261.png\n",
            "Saved cropped image: truncated_captchas\\captcha_262.png\n",
            "Saved cropped image: truncated_captchas\\captcha_263.png\n",
            "Saved cropped image: truncated_captchas\\captcha_264.png\n",
            "Saved cropped image: truncated_captchas\\captcha_265.png\n",
            "Saved cropped image: truncated_captchas\\captcha_266.png\n",
            "Saved cropped image: truncated_captchas\\captcha_267.png\n",
            "Saved cropped image: truncated_captchas\\captcha_268.png\n",
            "Saved cropped image: truncated_captchas\\captcha_269.png\n",
            "Saved cropped image: truncated_captchas\\captcha_27.png\n",
            "Saved cropped image: truncated_captchas\\captcha_270.png\n",
            "Saved cropped image: truncated_captchas\\captcha_271.png\n",
            "Saved cropped image: truncated_captchas\\captcha_272.png\n",
            "Saved cropped image: truncated_captchas\\captcha_273.png\n",
            "Saved cropped image: truncated_captchas\\captcha_274.png\n",
            "Saved cropped image: truncated_captchas\\captcha_275.png\n",
            "Saved cropped image: truncated_captchas\\captcha_276.png\n",
            "Saved cropped image: truncated_captchas\\captcha_277.png\n",
            "Saved cropped image: truncated_captchas\\captcha_278.png\n",
            "Saved cropped image: truncated_captchas\\captcha_279.png\n",
            "Saved cropped image: truncated_captchas\\captcha_28.png\n",
            "Saved cropped image: truncated_captchas\\captcha_280.png\n",
            "Saved cropped image: truncated_captchas\\captcha_281.png\n",
            "Saved cropped image: truncated_captchas\\captcha_282.png\n",
            "Saved cropped image: truncated_captchas\\captcha_283.png\n",
            "Saved cropped image: truncated_captchas\\captcha_284.png\n",
            "Saved cropped image: truncated_captchas\\captcha_285.png\n",
            "Saved cropped image: truncated_captchas\\captcha_286.png\n",
            "Saved cropped image: truncated_captchas\\captcha_287.png\n",
            "Saved cropped image: truncated_captchas\\captcha_288.png\n",
            "Saved cropped image: truncated_captchas\\captcha_289.png\n",
            "Saved cropped image: truncated_captchas\\captcha_29.png\n",
            "Saved cropped image: truncated_captchas\\captcha_290.png\n",
            "Saved cropped image: truncated_captchas\\captcha_291.png\n",
            "Saved cropped image: truncated_captchas\\captcha_292.png\n",
            "Saved cropped image: truncated_captchas\\captcha_293.png\n",
            "Saved cropped image: truncated_captchas\\captcha_294.png\n",
            "Saved cropped image: truncated_captchas\\captcha_295.png\n",
            "Saved cropped image: truncated_captchas\\captcha_296.png\n",
            "Saved cropped image: truncated_captchas\\captcha_297.png\n",
            "Saved cropped image: truncated_captchas\\captcha_298.png\n",
            "Saved cropped image: truncated_captchas\\captcha_299.png\n",
            "Saved cropped image: truncated_captchas\\captcha_3.png\n",
            "Saved cropped image: truncated_captchas\\captcha_30.png\n",
            "Saved cropped image: truncated_captchas\\captcha_300.png\n",
            "Saved cropped image: truncated_captchas\\captcha_301.png\n",
            "Saved cropped image: truncated_captchas\\captcha_302.png\n",
            "Saved cropped image: truncated_captchas\\captcha_303.png\n",
            "Saved cropped image: truncated_captchas\\captcha_304.png\n",
            "Saved cropped image: truncated_captchas\\captcha_305.png\n",
            "Saved cropped image: truncated_captchas\\captcha_306.png\n",
            "Saved cropped image: truncated_captchas\\captcha_307.png\n",
            "Saved cropped image: truncated_captchas\\captcha_308.png\n",
            "Saved cropped image: truncated_captchas\\captcha_309.png\n",
            "Saved cropped image: truncated_captchas\\captcha_31.png\n",
            "Saved cropped image: truncated_captchas\\captcha_310.png\n",
            "Saved cropped image: truncated_captchas\\captcha_311.png\n",
            "Saved cropped image: truncated_captchas\\captcha_312.png\n",
            "Saved cropped image: truncated_captchas\\captcha_313.png\n",
            "Saved cropped image: truncated_captchas\\captcha_314.png\n",
            "Saved cropped image: truncated_captchas\\captcha_315.png\n",
            "Saved cropped image: truncated_captchas\\captcha_316.png\n",
            "Saved cropped image: truncated_captchas\\captcha_32.png\n",
            "Saved cropped image: truncated_captchas\\captcha_33.png\n",
            "Saved cropped image: truncated_captchas\\captcha_34.png\n",
            "Saved cropped image: truncated_captchas\\captcha_35.png\n",
            "Saved cropped image: truncated_captchas\\captcha_36.png\n",
            "Saved cropped image: truncated_captchas\\captcha_37.png\n",
            "Saved cropped image: truncated_captchas\\captcha_38.png\n",
            "Saved cropped image: truncated_captchas\\captcha_39.png\n",
            "Saved cropped image: truncated_captchas\\captcha_4.png\n",
            "Saved cropped image: truncated_captchas\\captcha_40.png\n",
            "Saved cropped image: truncated_captchas\\captcha_41.png\n",
            "Saved cropped image: truncated_captchas\\captcha_42.png\n",
            "Saved cropped image: truncated_captchas\\captcha_43.png\n",
            "Saved cropped image: truncated_captchas\\captcha_44.png\n",
            "Saved cropped image: truncated_captchas\\captcha_45.png\n",
            "Saved cropped image: truncated_captchas\\captcha_46.png\n",
            "Saved cropped image: truncated_captchas\\captcha_47.png\n",
            "Saved cropped image: truncated_captchas\\captcha_48.png\n",
            "Saved cropped image: truncated_captchas\\captcha_49.png\n",
            "Saved cropped image: truncated_captchas\\captcha_5.png\n",
            "Saved cropped image: truncated_captchas\\captcha_50.png\n",
            "Saved cropped image: truncated_captchas\\captcha_51.png\n",
            "Saved cropped image: truncated_captchas\\captcha_52.png\n",
            "Saved cropped image: truncated_captchas\\captcha_53.png\n",
            "Saved cropped image: truncated_captchas\\captcha_54.png\n",
            "Saved cropped image: truncated_captchas\\captcha_55.png\n",
            "Saved cropped image: truncated_captchas\\captcha_56.png\n",
            "Saved cropped image: truncated_captchas\\captcha_57.png\n",
            "Saved cropped image: truncated_captchas\\captcha_58.png\n",
            "Saved cropped image: truncated_captchas\\captcha_59.png\n",
            "Saved cropped image: truncated_captchas\\captcha_6.png\n",
            "Saved cropped image: truncated_captchas\\captcha_60.png\n",
            "Saved cropped image: truncated_captchas\\captcha_61.png\n",
            "Saved cropped image: truncated_captchas\\captcha_62.png\n",
            "Saved cropped image: truncated_captchas\\captcha_63.png\n",
            "Saved cropped image: truncated_captchas\\captcha_64.png\n",
            "Saved cropped image: truncated_captchas\\captcha_65.png\n",
            "Saved cropped image: truncated_captchas\\captcha_66.png\n",
            "Saved cropped image: truncated_captchas\\captcha_67.png\n",
            "Saved cropped image: truncated_captchas\\captcha_68.png\n",
            "Saved cropped image: truncated_captchas\\captcha_69.png\n",
            "Saved cropped image: truncated_captchas\\captcha_7.png\n",
            "Saved cropped image: truncated_captchas\\captcha_70.png\n",
            "Saved cropped image: truncated_captchas\\captcha_71.png\n",
            "Saved cropped image: truncated_captchas\\captcha_72.png\n",
            "Saved cropped image: truncated_captchas\\captcha_73.png\n",
            "Saved cropped image: truncated_captchas\\captcha_74.png\n",
            "Saved cropped image: truncated_captchas\\captcha_75.png\n",
            "Saved cropped image: truncated_captchas\\captcha_76.png\n",
            "Saved cropped image: truncated_captchas\\captcha_77.png\n",
            "Saved cropped image: truncated_captchas\\captcha_78.png\n",
            "Saved cropped image: truncated_captchas\\captcha_79.png\n",
            "Saved cropped image: truncated_captchas\\captcha_8.png\n",
            "Saved cropped image: truncated_captchas\\captcha_80.png\n",
            "Saved cropped image: truncated_captchas\\captcha_81.png\n",
            "Saved cropped image: truncated_captchas\\captcha_82.png\n",
            "Saved cropped image: truncated_captchas\\captcha_83.png\n",
            "Saved cropped image: truncated_captchas\\captcha_84.png\n",
            "Saved cropped image: truncated_captchas\\captcha_85.png\n",
            "Saved cropped image: truncated_captchas\\captcha_86.png\n",
            "Saved cropped image: truncated_captchas\\captcha_87.png\n",
            "Saved cropped image: truncated_captchas\\captcha_88.png\n",
            "Saved cropped image: truncated_captchas\\captcha_89.png\n",
            "Saved cropped image: truncated_captchas\\captcha_9.png\n",
            "Saved cropped image: truncated_captchas\\captcha_90.png\n",
            "Saved cropped image: truncated_captchas\\captcha_91.png\n",
            "Saved cropped image: truncated_captchas\\captcha_92.png\n",
            "Saved cropped image: truncated_captchas\\captcha_93.png\n",
            "Saved cropped image: truncated_captchas\\captcha_94.png\n",
            "Saved cropped image: truncated_captchas\\captcha_95.png\n",
            "Saved cropped image: truncated_captchas\\captcha_96.png\n",
            "Saved cropped image: truncated_captchas\\captcha_97.png\n",
            "Saved cropped image: truncated_captchas\\captcha_98.png\n",
            "Saved cropped image: truncated_captchas\\captcha_99.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def crop_and_save_images(input_folder, output_folder=\"truncated_captchas\",nbPixelsHaut= 330, nbPixelsBas= 55, nbPixelsGauche= 20, nbPixelsDroite = 320):\n",
        "    \"\"\"\n",
        "    Crops images in the input_folder according to predefined pixel boundaries\n",
        "    and saves them to the output_folder with the same filenames.\n",
        "\n",
        "    Args:\n",
        "        input_folder (str): Path to the folder containing the original images.\n",
        "        output_folder (str): Path where the cropped images will be saved.\n",
        "    \"\"\"\n",
        "    # Define cropping boundaries\n",
        "    \n",
        "\n",
        "    # Create output folder if it does not exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Process each image in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(input_path)\n",
        "        if image is None:\n",
        "            print(f\"Skipping {filename} (could not load image)\")\n",
        "            continue\n",
        "\n",
        "        # Apply cropping\n",
        "        cropped_image = image[nbPixelsBas:nbPixelsHaut, nbPixelsGauche:nbPixelsDroite]\n",
        "\n",
        "        # Save the cropped image to the new folder\n",
        "        cv2.imwrite(output_path, cropped_image)\n",
        "        print(f\"Saved cropped image: {output_path}\")\n",
        "\n",
        "# nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 50,10,190,230 # Premier perso\n",
        "# nbPixelsHaut, nbPixelsBas, nbPixelsGauche, nbPixelsDroite = 50,10, 265, 305 #Second perso\n",
        "\n",
        "# Example Usage\n",
        "nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 310,100,65,275 # Images tronquées\n",
        "crop_and_save_images(\"extracted_captchas/captchas_saved\",\"truncated_captchas\",nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite) #truncated captchas\n",
        "# nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 55,5,185,235\n",
        "# crop_and_save_images(\"extracted_captchas/captchas_saved\",\"premier_perso\",nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite) #premier perso\n",
        "# nbPixelsHaut, nbPixelsBas, nbPixelsGauche, nbPixelsDroite = 55,5, 260, 310\n",
        "# crop_and_save_images(\"extracted_captchas/captchas_saved\",\"second_perso\",nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite) #second perso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LuaXvUTGJGhd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Path to the folder containing the images\n",
        "image_folder = \"truncated_captchas\"  # Change this to your actual folder path\n",
        "\n",
        "# List all image files in the folder\n",
        "image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]  # Change extension if needed\n",
        "\n",
        "# Initialize an accumulator with zeros (assuming images are the same size)\n",
        "num_images = len(image_files)\n",
        "if num_images == 0:\n",
        "    raise ValueError(\"No images found in the specified folder.\")\n",
        "\n",
        "# Load the first image to get dimensions\n",
        "first_image = cv2.imread(os.path.join(image_folder, image_files[0]), cv2.IMREAD_COLOR)\n",
        "h, w, c = first_image.shape\n",
        "mean_image = np.zeros((h, w, c), dtype=np.float32)\n",
        "\n",
        "# Compute the sum of all images\n",
        "for file in image_files:\n",
        "    img = cv2.imread(os.path.join(image_folder, file), cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        print(f\"Warning: Could not read {file}\")\n",
        "        continue\n",
        "    mean_image += img.astype(np.float32)  # Convert to float to prevent overflow\n",
        "\n",
        "# Compute the mean by dividing by the number of images\n",
        "mean_image /= num_images\n",
        "\n",
        "# Convert back to uint8 format for visualization and saving\n",
        "mean_image = np.clip(mean_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Save the mean image\n",
        "cv2.imwrite(\"mean_image.png\", mean_image)\n",
        "\n",
        "# Display the mean image (optional)\n",
        "cv2.imshow(\"Mean Image\", mean_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned images saved in filtered_truncated_captchas\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "image_folder = \"truncated_captchas\"  # Change this to your actual folder\n",
        "output_folder = \"filtered_truncated_captchas\"  # Folder to save cleaned images\n",
        "mean_image_path = \"mean_image.png\"  # Path to saved mean image\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load the mean image\n",
        "mean_image = cv2.imread(mean_image_path, cv2.IMREAD_COLOR).astype(np.float32)\n",
        "\n",
        "# List all image files in the folder\n",
        "image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]  # Change extension if needed\n",
        "\n",
        "# Process each image\n",
        "for file in image_files:\n",
        "    img_path = os.path.join(image_folder, file)\n",
        "    output_path = os.path.join(output_folder, file)\n",
        "    \n",
        "    # Load the image\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float32)\n",
        "\n",
        "    # Subtract the mean image\n",
        "    cleaned_img = img - mean_image\n",
        "\n",
        "    # Normalize back to 0-255\n",
        "    cleaned_img = np.clip(cleaned_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Save the cleaned image\n",
        "    cv2.imwrite(output_path, cleaned_img)\n",
        "\n",
        "print(f\"Cleaned images saved in {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground truth coordinates: x1=40, y1=75, x2=115, y2=107\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def draw_labels_on_image(image_path, labels_file):\n",
        "    \"\"\"\n",
        "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
        "    and displays the image with red circles at those coordinates.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image.\n",
        "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Unable to load image: {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
        "    image_resized = image#cv2.resize(image, (340, 410))\n",
        "\n",
        "    # Read labels from the file\n",
        "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
        "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
        "\n",
        "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
        "    if 'img_name' in labels_df.columns:\n",
        "        row = labels_df[labels_df['img_name'] == image_name]\n",
        "    else:\n",
        "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
        "\n",
        "    if row.empty:\n",
        "        print(f\"No labels found for {image_name}\")\n",
        "        return\n",
        "\n",
        "    # Extract ground truth coordinates\n",
        "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
        "    x1 = int(x1)\n",
        "    x2 = int(x2)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "    # Draw red circles at the ground truth coordinates\n",
        "    image_drawn = image_resized.copy()\n",
        "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
        "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
        "\n",
        "    # Display the image with the drawn points\n",
        "    cv2.imshow(\"img\",image_drawn)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "image_path = f\"filtered_truncated_captchas/captcha_8.png\"\n",
        "labels_file = \"truncated_labels.csv\"\n",
        "draw_labels_on_image(image_path,labels_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing mean & std: 100%|██████████| 316/316 [00:00<00:00, 2028.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "moyenne = [0.83977205 0.8524061  0.55467314] \n",
            " std=[0.2027646  0.18541439 0.18301369]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Normalizing images: 100%|██████████| 316/316 [00:00<00:00, 346.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Normalized images saved in: normalized_images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "def compute_mean_std(input_folder):\n",
        "    \"\"\"Compute the mean and std for each RGB channel across all images.\"\"\"\n",
        "    image_list = []\n",
        "    \n",
        "    for filename in tqdm(os.listdir(input_folder), desc=\"Computing mean & std\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # Read image in BGR format\n",
        "        if img is not None:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "            image_list.append(img)\n",
        "\n",
        "    # Convert list to a big numpy array (N, H, W, C)\n",
        "    image_array = np.stack(image_list, axis=0).astype(np.float32) / 255.0\n",
        "\n",
        "    # Compute mean and std along (N, H, W) axis → (C,)\n",
        "    mean = np.mean(image_array, axis=(0, 1, 2))\n",
        "    std = np.std(image_array, axis=(0, 1, 2))\n",
        "    print(f\"moyenne = {mean} \\n std={std}\")\n",
        "    return mean, std\n",
        "\n",
        "def normalize_images(input_folder, output_folder):\n",
        "    \"\"\"Normalize RGB channels of all images in input_folder and save them to output_folder.\"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Create output folder if not exists\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean, std = compute_mean_std(input_folder)\n",
        "\n",
        "    for filename in tqdm(os.listdir(input_folder), desc=\"Normalizing images\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # Read image in BGR format\n",
        "        if img is None:\n",
        "            print(f\"Skipping {filename} (unable to read)\")\n",
        "            continue\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        img = img.astype(np.float32) / 255.0  # Scale to [0,1]\n",
        "\n",
        "        # Normalize: (pixel - mean) / std\n",
        "        img = (img - mean) / std\n",
        "\n",
        "        # Convert back to 0-255 range for saving\n",
        "        img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)\n",
        "\n",
        "        # Save processed image\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))  # Convert back to BGR\n",
        "\n",
        "    print(f\"✅ Normalized images saved in: {output_folder}\")\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "input_folder = \"truncated_captchas\"\n",
        "output_folder = \"normalized_images\"\n",
        "normalize_images(input_folder, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing new images: 100%|██████████| 316/316 [00:00<00:00, 1474.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Normalized images saved in: normalized_premier_perso\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing new images: 100%|██████████| 316/316 [00:00<00:00, 1164.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Normalized images saved in: normalized_second_perso\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "def apply_existing_normalization(input_folder, output_folder, mean, std):\n",
        "    \"\"\"\n",
        "    Applies the same mean and std normalization (computed earlier) to new images.\n",
        "\n",
        "    Args:\n",
        "        input_folder (str): Path to the folder containing new images.\n",
        "        output_folder (str): Path to save the normalized images.\n",
        "        mean (tuple): Mean values of the original dataset (R, G, B).\n",
        "        std (tuple): Standard deviation values of the original dataset (R, G, B).\n",
        "    \"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
        "    df = pd.DataFrame([])\n",
        "    for filename in tqdm(os.listdir(input_folder), desc=\"Processing new images\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # Read image in BGR format\n",
        "        if img is None:\n",
        "            print(f\"Skipping {filename} (unable to read)\")\n",
        "            continue\n",
        "        \n",
        "        # Convert to RGB and scale pixel values to [0,1]\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "        # Apply normalization using precomputed mean & std\n",
        "        img_normalized = (img - mean) / std  # Standardize the image\n",
        "\n",
        "        # Rescale to 0-255 for saving\n",
        "        img_normalized = ((img_normalized - img_normalized.min()) / (img_normalized.max() - img_normalized.min()) * 255).astype(np.uint8)\n",
        "        df = pd.concat([df, pd.DataFrame({\"img_name\": [filename], \"min\": [img_normalized.min()], \"max\": [img_normalized.max()]})], ignore_index=True)\n",
        "\n",
        "        # Convert back to BGR before saving (OpenCV expects BGR format)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        cv2.imwrite(output_path, cv2.cvtColor(img_normalized, cv2.COLOR_RGB2BGR))\n",
        "    df.to_csv(input_folder+\".csv\")\n",
        "    print(f\"✅ Normalized images saved in: {output_folder}\")\n",
        "\n",
        "# Valeurs trouvées:\n",
        "mean = (0.83977205, 0.8524061, 0.55467314)  # Dataset's mean\n",
        "std = (0.2027646, 0.18541439, 0.18301369)  # Dataset's std\n",
        "\n",
        "input_folder = \"premier_perso\"\n",
        "output_folder = \"normalized_premier_perso\"\n",
        "apply_existing_normalization(input_folder, output_folder, mean, std)\n",
        "\n",
        "input_folder = \"second_perso\"\n",
        "output_folder = \"normalized_second_perso\"\n",
        "apply_existing_normalization(input_folder, output_folder, mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def encode_image(img, original_min, original_max):\n",
        "    \"\"\"\n",
        "    Reverts the min-max scaling applied to an image.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): The encoded image (after min-max scaling to 255).\n",
        "        original_min (float): The minimum pixel value before encoding.\n",
        "        original_max (float): The maximum pixel value before encoding.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The encoded image with values standardized\n",
        "    \"\"\"\n",
        "    img = img.astype(np.float32)  # Convert to float32 for precision\n",
        "\n",
        "    # Reverse min-max scaling: \n",
        "    img_encoded = img / 255.0 * (original_max - original_min) + original_min\n",
        "\n",
        "    return img_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>x1</th>\n",
              "      <th>y1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>captcha_1.png</td>\n",
              "      <td>-1.033635</td>\n",
              "      <td>0.793751</td>\n",
              "      <td>-0.911822</td>\n",
              "      <td>-1.717953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>captcha_10.png</td>\n",
              "      <td>-0.877801</td>\n",
              "      <td>0.750201</td>\n",
              "      <td>0.950419</td>\n",
              "      <td>-0.778652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>captcha_100.png</td>\n",
              "      <td>0.101729</td>\n",
              "      <td>1.708298</td>\n",
              "      <td>-1.505283</td>\n",
              "      <td>1.612295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>captcha_101.png</td>\n",
              "      <td>0.880900</td>\n",
              "      <td>0.380028</td>\n",
              "      <td>-0.809501</td>\n",
              "      <td>1.099949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>captcha_102.png</td>\n",
              "      <td>-0.054105</td>\n",
              "      <td>0.488902</td>\n",
              "      <td>0.950419</td>\n",
              "      <td>1.206688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>captcha_95.png</td>\n",
              "      <td>-0.298988</td>\n",
              "      <td>-1.383742</td>\n",
              "      <td>0.336493</td>\n",
              "      <td>1.441513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>captcha_96.png</td>\n",
              "      <td>-0.989111</td>\n",
              "      <td>-1.274867</td>\n",
              "      <td>-1.095999</td>\n",
              "      <td>1.249383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>captcha_97.png</td>\n",
              "      <td>-1.055897</td>\n",
              "      <td>1.272800</td>\n",
              "      <td>0.643456</td>\n",
              "      <td>-0.287654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>captcha_98.png</td>\n",
              "      <td>-1.055897</td>\n",
              "      <td>-0.011921</td>\n",
              "      <td>1.339238</td>\n",
              "      <td>-1.120216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>captcha_99.png</td>\n",
              "      <td>-1.122683</td>\n",
              "      <td>1.599424</td>\n",
              "      <td>-1.484819</td>\n",
              "      <td>0.181996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>316 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            img_name        x1        y1        x2        y2\n",
              "0      captcha_1.png -1.033635  0.793751 -0.911822 -1.717953\n",
              "1     captcha_10.png -0.877801  0.750201  0.950419 -0.778652\n",
              "2    captcha_100.png  0.101729  1.708298 -1.505283  1.612295\n",
              "3    captcha_101.png  0.880900  0.380028 -0.809501  1.099949\n",
              "4    captcha_102.png -0.054105  0.488902  0.950419  1.206688\n",
              "..               ...       ...       ...       ...       ...\n",
              "311   captcha_95.png -0.298988 -1.383742  0.336493  1.441513\n",
              "312   captcha_96.png -0.989111 -1.274867 -1.095999  1.249383\n",
              "313   captcha_97.png -1.055897  1.272800  0.643456 -0.287654\n",
              "314   captcha_98.png -1.055897 -0.011921  1.339238 -1.120216\n",
              "315   captcha_99.png -1.122683  1.599424 -1.484819  0.181996\n",
              "\n",
              "[316 rows x 5 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def standardize_columns(df, columns, output_file=\"mean_std.txt\"):\n",
        "    \"\"\"\n",
        "    Standardizes the selected columns of a DataFrame and saves the mean and std for each column in a text file.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        columns (list): List of column names to standardize.\n",
        "        output_file (str): File name to save the mean and std values.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with standardized columns.\n",
        "    \"\"\"\n",
        "    mean_std_values = {}\n",
        "\n",
        "    for col in columns:\n",
        "        mean = df[col].mean()\n",
        "        std = df[col].std()\n",
        "\n",
        "        if std == 0:  # Avoid division by zero\n",
        "            std = 1  \n",
        "\n",
        "        df[col] = (df[col] - mean) / std\n",
        "        mean_std_values[col] = {\"mean\": mean, \"std\": std}\n",
        "\n",
        "    # Save mean and std to a text file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for col, values in mean_std_values.items():\n",
        "            f.write(f\"{col} mean: {values['mean']}, std: {values['std']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df = pd.read_csv(\"truncated_labels.csv\")\n",
        "df = standardize_columns(df,df.columns[1:])\n",
        "df.to_csv(\"std_truncated_labels.csv\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 210, 3)\n",
            "(50, 50, 3)\n",
            "(50, 50, 3)\n"
          ]
        }
      ],
      "source": [
        "extract_dirs = ['normalized_images',\"normalized_premier_perso\",\"normalized_second_perso\"]\n",
        "for extract_dir in extract_dirs:\n",
        "    image_path = f\"{extract_dir}/captcha_8.png\"\n",
        "    image = cv2.imread(image_path)\n",
        "    print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Large image shape: torch.Size([8, 3, 210, 210])\n",
            "Small image 1 shape: torch.Size([8, 3, 50, 50])\n",
            "Small image 2 shape: torch.Size([8, 3, 50, 50])\n",
            "Labels: torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class CaptchaDataset(Dataset):\n",
        "    def __init__(self, extract_dirs, label_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            extract_dirs (list of str): List of directories where images are stored.\n",
        "            label_file (str): Path to the CSV file containing labels.\n",
        "            transform (callable, optional): Transformations to apply on images.\n",
        "        \"\"\"\n",
        "        self.extract_dirs = extract_dirs  # List of directories\n",
        "        self.labels = pd.read_csv(label_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def normalize_image(self,image, mean=(0.83977205, 0.8524061, 0.55467314), std=(0.2027646, 0.18541439,0.18301369)):\n",
        "        \"\"\"\n",
        "        Normalize the image with given mean and std for each channel.\n",
        "        \n",
        "        Args:\n",
        "            image (np.array): The input image to be normalized.\n",
        "            mean (tuple): A tuple containing the mean for each channel (R, G, B).\n",
        "            std (tuple): A tuple containing the standard deviation for each channel (R, G, B).\n",
        "            \n",
        "        Returns:\n",
        "            np.array: The normalized image.\n",
        "        \"\"\"\n",
        "        # Normalize image by subtracting mean and dividing by std for each channel (RGB)\n",
        "        image = (image - mean) / std\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image ID\n",
        "        img_id = self.labels.iloc[idx]['img_name']\n",
        "        \n",
        "        # Initialize paths for the three parts\n",
        "        large_img_path = None\n",
        "        small_img1_path = None\n",
        "        small_img2_path = None\n",
        "\n",
        "        # Construct the paths for the images\n",
        "        large_img_path = os.path.join(self.extract_dirs[0], img_id)\n",
        "        small_img1_path = os.path.join(self.extract_dirs[1], img_id)\n",
        "        small_img2_path = os.path.join(self.extract_dirs[2], img_id)\n",
        "\n",
        "        # Check if we found all parts\n",
        "        if not os.path.exists(large_img_path) or not os.path.exists(small_img1_path) or not os.path.exists(small_img2_path):\n",
        "            print(f\"Missing images for {img_id}!\")\n",
        "            return None\n",
        "\n",
        "        # Load images (big and small)\n",
        "        large_img = cv2.imread(large_img_path)\n",
        "        small_img1 = cv2.imread(small_img1_path)\n",
        "        small_img2 = cv2.imread(small_img2_path)\n",
        "\n",
        "        # Convert to RGB (OpenCV loads as BGR by default)\n",
        "        large_img = cv2.cvtColor(large_img, cv2.COLOR_BGR2RGB)\n",
        "        small_img1 = cv2.cvtColor(small_img1, cv2.COLOR_BGR2RGB)\n",
        "        small_img2 = cv2.cvtColor(small_img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Apply normalization using the provided mean and std\n",
        "        large_img = self.normalize_image(large_img)\n",
        "        small_img1 = self.normalize_image(small_img1)\n",
        "        small_img2 = self.normalize_image(small_img2)\n",
        "\n",
        "        # Transpose to PyTorch format (C, H, W)\n",
        "        large_img = np.transpose(large_img, (2, 0, 1))\n",
        "        small_img1 = np.transpose(small_img1, (2, 0, 1))\n",
        "        small_img2 = np.transpose(small_img2, (2, 0, 1))\n",
        "\n",
        "        # Get labels (x1, y1, x2, y2)\n",
        "        labels = self.labels.iloc[idx][['x1', 'y1', 'x2', 'y2']].values.astype(np.float32)\n",
        "\n",
        "        # Convert to tensors\n",
        "        large_img = torch.tensor(large_img)\n",
        "        small_img1 = torch.tensor(small_img1)\n",
        "        small_img2 = torch.tensor(small_img2)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        return large_img, small_img1, small_img2, labels\n",
        "\n",
        "# Example Usage\n",
        "extract_dirs = [\"normalized_images\", \"normalized_premier_perso\", \"normalized_second_perso\"]\n",
        "label_file = \"std_truncated_labels.csv\"\n",
        "\n",
        "dataset = CaptchaDataset(extract_dirs, label_file)\n",
        "train_loader = DataLoader(dataset, batch_size=8,shuffle=True)\n",
        "\n",
        "# Test data loading\n",
        "for large_img, small_img1, small_img2, labels in train_loader:\n",
        "    if large_img is not None:\n",
        "        print(\"Large image shape:\", large_img.shape)   # Expected: (8, 3, 210, 210)\n",
        "        print(\"Small image 1 shape:\", small_img1.shape)  # Expected: (8, 3, 50, 50)\n",
        "        print(\"Small image 2 shape:\", small_img2.shape)  # Expected: (8, 3, 50, 50)\n",
        "        print(\"Labels:\", labels.shape)  # Expected: (8, 4)\n",
        "    break  # Just checking one batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CaptchaClickCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CaptchaClickCNN, self).__init__()\n",
        "\n",
        "        # Define the CNN layers for the large image (210, 210, 3)\n",
        "        self.large_img_cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Define the CNN layers for the small image (50, 50, 3)\n",
        "        self.small_img_cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # We'll calculate the exact feature dimensions in the forward pass\n",
        "        # and lazily initialize the fully connected layers later\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        self.initialized = False\n",
        "\n",
        "    def _initialize_fc_layers(self, input_size):\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 4)\n",
        "        self.initialized = True\n",
        "\n",
        "    def forward(self, large_img, small_img1, small_img2):\n",
        "        # Ensure input images are float32 for consistency\n",
        "        large_img = large_img.float()\n",
        "        small_img1 = small_img1.float()\n",
        "        small_img2 = small_img2.float()\n",
        "\n",
        "        # Pass large image through its CNN layers\n",
        "        large_img_features = self.large_img_cnn(large_img)\n",
        "        \n",
        "        # Pass small images through their CNN layers\n",
        "        small_img1_features = self.small_img_cnn(small_img1)\n",
        "        small_img2_features = self.small_img_cnn(small_img2)\n",
        "\n",
        "        # Flatten the CNN outputs\n",
        "        large_img_features = large_img_features.view(large_img_features.size(0), -1)\n",
        "        small_img1_features = small_img1_features.view(small_img1_features.size(0), -1)\n",
        "        small_img2_features = small_img2_features.view(small_img2_features.size(0), -1)\n",
        "\n",
        "        # Concatenate the features from all images\n",
        "        combined_features = torch.cat((large_img_features, small_img1_features, small_img2_features), dim=1)\n",
        "        \n",
        "        # Lazy initialization of FC layers on first forward pass\n",
        "        if not self.initialized:\n",
        "            self._initialize_fc_layers(combined_features.shape[1])\n",
        "            \n",
        "        # Pass through fully connected layers\n",
        "        x = F.relu(self.fc1(combined_features))\n",
        "        x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        output = self.fc3(x)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Example usage with dimension tracking\n",
        "def test_dimensions():\n",
        "    # Create sample inputs\n",
        "    batch_size = 8\n",
        "    large_img = torch.randn(batch_size, 3, 210, 210)\n",
        "    small_img1 = torch.randn(batch_size, 3, 50, 50)\n",
        "    small_img2 = torch.randn(batch_size, 3, 50, 50)\n",
        "    \n",
        "    # Initialize the model\n",
        "    model = CaptchaClickCNN()\n",
        "    \n",
        "    # Forward pass with print statements to track dimensions\n",
        "    print(f\"Large image input shape: {large_img.shape}\")\n",
        "    large_features = model.large_img_cnn(large_img)\n",
        "    print(f\"Large image after CNN shape: {large_features.shape}\")\n",
        "    large_flattened = large_features.view(large_features.size(0), -1)\n",
        "    print(f\"Large image flattened shape: {large_flattened.shape}\")\n",
        "    \n",
        "    print(f\"\\nSmall image input shape: {small_img1.shape}\")\n",
        "    small_features = model.small_img_cnn(small_img1)\n",
        "    print(f\"Small image after CNN shape: {small_features.shape}\")\n",
        "    small_flattened = small_features.view(small_features.size(0), -1)\n",
        "    print(f\"Small image flattened shape: {small_flattened.shape}\")\n",
        "    \n",
        "    # Calculate total feature size\n",
        "    total_feature_size = large_flattened.shape[1] + small_flattened.shape[1] * 2\n",
        "    print(f\"\\nTotal feature size for FC layer: {total_feature_size}\")\n",
        "    \n",
        "    # Complete forward pass\n",
        "    output = model(large_img, small_img1, small_img2)\n",
        "    print(f\"\\nFinal output shape: {output.shape}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, train_loader, num_epochs=10, lr=1e-4):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for large_img, small_img1, small_img2, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(large_img, small_img1, small_img2)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# To use the model with your existing train_loader:\n",
        "# model = CaptchaClickCNN()\n",
        "# trained_model = train_model(model, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully from captcha_model.pth\n",
            "Model was saved after epoch 10\n",
            "Predicted coordinates: [-5.791699  10.908616   4.1962066 48.42848  ]\n",
            "Script executed successfully!\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "# Assuming CaptchaClickCNN is defined as in the previous artifact\n",
        "\n",
        "def save_model(model, path=\"captcha_model.pth\", save_optimizer=False, optimizer=None, epoch=None):\n",
        "    \"\"\"\n",
        "    Save the trained model along with optional training state.\n",
        "    \n",
        "    Args:\n",
        "        model: The trained CaptchaClickCNN model\n",
        "        path: Path to save the model\n",
        "        save_optimizer: Whether to save optimizer state for resuming training\n",
        "        optimizer: The optimizer used during training\n",
        "        epoch: Current epoch number\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'initialized': model.initialized,\n",
        "    }\n",
        "    \n",
        "    if save_optimizer and optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "    \n",
        "    if epoch is not None:\n",
        "        checkpoint['epoch'] = epoch\n",
        "    \n",
        "    print(f\"Saving model to {path}...\")\n",
        "    torch.save(checkpoint, path)\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "\n",
        "def load_model(path=\"captcha_model.pth\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
        "               load_optimizer=False, optimizer=None):\n",
        "    \"\"\"\n",
        "    Load a saved CaptchaClickCNN model.\n",
        "    \n",
        "    Args:\n",
        "        path: Path to the saved model\n",
        "        device: Device to load the model on ('cuda' or 'cpu')\n",
        "        load_optimizer: Whether to load optimizer state\n",
        "        optimizer: The optimizer to load state into\n",
        "        \n",
        "    Returns:\n",
        "        model: The loaded model\n",
        "        epoch: The epoch at which the model was saved (if saved)\n",
        "        optimizer: The loaded optimizer (if requested)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"No model found at {path}\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    \n",
        "    # Create a new model instance\n",
        "    model = CaptchaClickCNN()\n",
        "    \n",
        "    # Load model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    # Set initialization flag\n",
        "    model.initialized = checkpoint.get('initialized', False)\n",
        "    \n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    # Load optimizer if requested\n",
        "    if load_optimizer and 'optimizer_state_dict' in checkpoint and optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    epoch = checkpoint.get('epoch', None)\n",
        "    \n",
        "    print(f\"Model loaded successfully from {path}\")\n",
        "    if epoch is not None:\n",
        "        print(f\"Model was saved after epoch {epoch}\")\n",
        "    \n",
        "    return model, epoch, optimizer if load_optimizer else model\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Test the model on the test dataset and compute metrics.\n",
        "    \n",
        "    Args:\n",
        "        model: The trained CaptchaClickCNN model\n",
        "        test_loader: DataLoader containing test data\n",
        "        device: Device to run testing on\n",
        "        \n",
        "    Returns:\n",
        "        average_loss: Mean Square Error on test set\n",
        "        predictions: Model predictions\n",
        "        ground_truth: True labels\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    model = model.to(device)\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    print(\"Testing model...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with torch.no_grad():  # No need to track gradients during testing\n",
        "        for large_img, small_img1, small_img2, labels in test_loader:\n",
        "            # Move data to device\n",
        "            large_img = large_img.to(device)\n",
        "            small_img1 = small_img1.to(device)\n",
        "            small_img2 = small_img2.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(large_img, small_img1, small_img2)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * large_img.size(0)\n",
        "            \n",
        "            # Store predictions and labels\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate average loss\n",
        "    average_loss = total_loss / len(test_loader.dataset)\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    predictions = np.array(all_predictions)\n",
        "    ground_truth = np.array(all_labels)\n",
        "    \n",
        "    test_time = time.time() - start_time\n",
        "    print(f\"Testing completed in {test_time:.2f} seconds\")\n",
        "    print(f\"Average MSE Loss: {average_loss:.4f}\")\n",
        "    \n",
        "    # Calculate Euclidean distance error for click predictions\n",
        "    if predictions.shape[1] == 4:  # If we have x1, y1, x2, y2 coordinates\n",
        "        # Calculate Euclidean distance for first point (x1, y1)\n",
        "        dist1 = np.sqrt(np.square(predictions[:, 0] - ground_truth[:, 0]) + \n",
        "                        np.square(predictions[:, 1] - ground_truth[:, 1]))\n",
        "        \n",
        "        # Calculate Euclidean distance for second point (x2, y2)\n",
        "        dist2 = np.sqrt(np.square(predictions[:, 2] - ground_truth[:, 2]) + \n",
        "                        np.square(predictions[:, 3] - ground_truth[:, 3]))\n",
        "        \n",
        "        # Average distance error\n",
        "        avg_dist_error = (np.mean(dist1) + np.mean(dist2)) / 2\n",
        "        print(f\"Average Distance Error: {avg_dist_error:.2f} pixels\")\n",
        "    \n",
        "    return average_loss, predictions, ground_truth\n",
        "\n",
        "\n",
        "def visualize_predictions(model, sample_data, num_samples=5, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Visualize model predictions on sample images.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained CaptchaClickCNN model\n",
        "        sample_data: Sample data for visualization (large_img, small_img1, small_img2, coords)\n",
        "        num_samples: Number of samples to visualize\n",
        "        device: Device to run inference on\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    plt.figure(figsize=(15, num_samples * 5))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(min(num_samples, len(sample_data))):\n",
        "            large_img, small_img1, small_img2, true_coords = sample_data[i]\n",
        "            \n",
        "            # Add batch dimension\n",
        "            large_img = large_img.unsqueeze(0).to(device)\n",
        "            small_img1 = small_img1.unsqueeze(0).to(device)\n",
        "            small_img2 = small_img2.unsqueeze(0).to(device)\n",
        "            \n",
        "            # Get model prediction\n",
        "            pred_coords = model(large_img, small_img1, small_img2).cpu().numpy()[0]\n",
        "            \n",
        "            # Convert large image tensor to numpy for display\n",
        "            large_img_np = large_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "            \n",
        "            # Normalize image for display if needed\n",
        "            if large_img_np.max() > 1.0:\n",
        "                large_img_np = large_img_np / 255.0\n",
        "            \n",
        "            # Plot the image\n",
        "            plt.subplot(num_samples, 3, i*3 + 1)\n",
        "            plt.imshow(large_img_np)\n",
        "            plt.title(f\"Large Image {i+1}\")\n",
        "            \n",
        "            # Plot true coordinates\n",
        "            plt.plot(true_coords[0], true_coords[1], 'go', markersize=10, label='True Point 1')\n",
        "            plt.plot(true_coords[2], true_coords[3], 'bo', markersize=10, label='True Point 2')\n",
        "            \n",
        "            # Plot predicted coordinates  \n",
        "            plt.plot(pred_coords[0], pred_coords[1], 'rx', markersize=10, label='Pred Point 1')\n",
        "            plt.plot(pred_coords[2], pred_coords[3], 'mx', markersize=10, label='Pred Point 2')\n",
        "            \n",
        "            if i == 0:\n",
        "                plt.legend()\n",
        "            \n",
        "            # Display the small images\n",
        "            plt.subplot(num_samples, 3, i*3 + 2)\n",
        "            small_img1_np = small_img1[0].cpu().permute(1, 2, 0).numpy()\n",
        "            if small_img1_np.max() > 1.0:\n",
        "                small_img1_np = small_img1_np / 255.0\n",
        "            plt.imshow(small_img1_np)\n",
        "            plt.title(f\"Small Image 1\")\n",
        "            \n",
        "            plt.subplot(num_samples, 3, i*3 + 3)\n",
        "            small_img2_np = small_img2[0].cpu().permute(1, 2, 0).numpy()\n",
        "            if small_img2_np.max() > 1.0:\n",
        "                small_img2_np = small_img2_np / 255.0\n",
        "            plt.imshow(small_img2_np)\n",
        "            plt.title(f\"Small Image 2\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"captcha_predictions.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def inference_on_new_images(model, large_img_path, small_img1_path, small_img2_path, \n",
        "                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Run inference on new images loaded from disk.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained CaptchaClickCNN model\n",
        "        large_img_path: Path to large image file\n",
        "        small_img1_path: Path to first small image file\n",
        "        small_img2_path: Path to second small image file\n",
        "        device: Device to run inference on\n",
        "        \n",
        "    Returns:\n",
        "        predicted_coords: Predicted coordinates [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    def normalize_image(image, mean=(0.83977205, 0.8524061, 0.55467314), std=(0.2027646, 0.18541439, 0.18301369)):\n",
        "        # Convert the image to a NumPy array\n",
        "        image = np.array(image).astype(np.float32)\n",
        "        \n",
        "        # Normalize image by subtracting mean and dividing by std for each channel (RGB)\n",
        "        image = (image - mean) / std\n",
        "        \n",
        "        return image\n",
        "\n",
        "    # Load and preprocess images\n",
        "    def load_and_preprocess(img_path, target_size):\n",
        "        # Open the image, convert it to RGB\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Resize the image to the target size\n",
        "        img = img.resize(target_size)\n",
        "        \n",
        "        # Normalize the image\n",
        "        img = normalize_image(img)\n",
        "        \n",
        "        # Convert to torch tensor (channel-first format: C x H x W)\n",
        "        img_tensor = torch.FloatTensor(img).permute(2, 0, 1)  # Convert from HWC to CHW\n",
        "        return img_tensor\n",
        "    \n",
        "    # Load and preprocess all images\n",
        "    large_img = load_and_preprocess(large_img_path, (210, 210))\n",
        "    small_img1 = load_and_preprocess(small_img1_path, (50, 50))\n",
        "    small_img2 = load_and_preprocess(small_img2_path, (50, 50))\n",
        "    \n",
        "    # Add batch dimension (make them 4D tensors)\n",
        "    large_img = large_img.unsqueeze(0).to(device)\n",
        "    small_img1 = small_img1.unsqueeze(0).to(device)\n",
        "    small_img2 = small_img2.unsqueeze(0).to(device)\n",
        "    \n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        predicted_coords = model(large_img, small_img1, small_img2).cpu().numpy()[0]\n",
        "    \n",
        "    print(\"Predicted coordinates:\", predicted_coords)\n",
        "    return predicted_coords\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy model for demonstration\n",
        "    model = CaptchaClickCNN()\n",
        "    \n",
        "    # Create optimizer for demonstration\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Example: Save model after training\n",
        "    # save_model(model, path=\"captcha_model.pth\", save_optimizer=True, optimizer=optimizer, epoch=10)\n",
        "    \n",
        "    # Example: Load model for inference\n",
        "    model = load_model(path=\"captcha_model.pth\")\n",
        "    \n",
        "    # Example: Test model (assuming you have a test_loader)\n",
        "    # test_loss, predictions, ground_truth = test_model(model, test_loader)\n",
        "    \n",
        "    # Example: Run inference on new images\n",
        "    predicted_coords = inference_on_new_images(\n",
        "        model[0], \n",
        "        large_img_path=\"normalized_images/captcha_1.png\",\n",
        "        small_img1_path=\"normalized_premier_perso/captcha_1.png\",\n",
        "        small_img2_path=\"normalized_second_perso/captcha_1.png\"\n",
        "    )\n",
        "    \n",
        "    print(\"Script executed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# First import your CaptchaClickCNN model definition\n",
        "# from captcha_model import CaptchaClickCNN\n",
        "\n",
        "# Sample Dataset class - replace with your actual dataset\n",
        "class CaptchaDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Dummy dataset - replace with your actual dataset loading logic\n",
        "        \n",
        "        Args:\n",
        "            data_dir: Directory containing your data\n",
        "            transform: Optional transforms to apply to images\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        # In a real scenario, you would load your data here\n",
        "        # For demonstration, we'll create dummy data\n",
        "        self.data_size = 100  # Number of samples\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # In a real scenario, you would load your images and labels here\n",
        "        # For demonstration, we'll create random data\n",
        "        \n",
        "        # Create dummy images\n",
        "        large_img = torch.randn(3, 210, 210)\n",
        "        small_img1 = torch.randn(3, 50, 50)\n",
        "        small_img2 = torch.randn(3, 50, 50)\n",
        "        \n",
        "        # Create dummy target coordinates\n",
        "        # Coordinates should be within the image dimensions\n",
        "        coords = torch.tensor([\n",
        "            np.random.randint(0, 210),  # x1\n",
        "            np.random.randint(0, 210),  # y1\n",
        "            np.random.randint(0, 210),  # x2\n",
        "            np.random.randint(0, 210)   # y2\n",
        "        ], dtype=torch.float32)\n",
        "        \n",
        "        return large_img, small_img1, small_img2, coords\n",
        "\n",
        "\n",
        "# Main workflow function\n",
        "def captcha_model_workflow():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Create dataset and dataloaders\n",
        "    # Replace with your actual dataset path\n",
        "    dataset = CaptchaDataset(data_dir=\"./data\")\n",
        "    \n",
        "    # Split dataset into train, validation, and test\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "    \n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    batch_size = 8\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "    \n",
        "    # Create model\n",
        "    model = CaptchaClickCNN().to(device)\n",
        "    \n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Training parameters\n",
        "    num_epochs = 20\n",
        "    best_val_loss = float('inf')\n",
        "    model_save_path = \"captcha_model_best.pth\"\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        for large_img, small_img1, small_img2, labels in train_loader:\n",
        "            # Move data to device\n",
        "            large_img = large_img.to(device)\n",
        "            small_img1 = small_img1.to(device)\n",
        "            small_img2 = small_img2.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(large_img, small_img1, small_img2)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item() * large_img.size(0)\n",
        "        \n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for large_img, small_img1, small_img2, labels in val_loader:\n",
        "                # Move data to device\n",
        "                large_img = large_img.to(device)\n",
        "                small_img1 = small_img1.to(device)\n",
        "                small_img2 = small_img2.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = model(large_img, small_img1, small_img2)\n",
        "                \n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * large_img.size(0)\n",
        "        \n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        \n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}\")\n",
        "        \n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_model(model, model_save_path, save_optimizer=True, \n",
        "                      optimizer=optimizer, epoch=epoch)\n",
        "            print(f\"Saved best model so far with validation loss: {val_loss:.4f}\")\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    \n",
        "    # Load the best model for testing\n",
        "    model, _, _ = load_model(model_save_path, device=device)\n",
        "    \n",
        "    # Test the model\n",
        "    test_loss, predictions, ground_truth = test_model(model, test_loader, device=device)\n",
        "    \n",
        "    # Save some sample test data for visualization\n",
        "    sample_data = []\n",
        "    for i, (large_img, small_img1, small_img2, labels) in enumerate(test_loader):\n",
        "        if i >= 5:  # Get 5 samples\n",
        "            break\n",
        "        sample_data.append((large_img[0], small_img1[0], small_img2[0], labels[0]))\n",
        "    \n",
        "    # Visualize predictions\n",
        "    visualize_predictions(model, sample_data, device=device)\n",
        "    \n",
        "    print(\"Workflow completed successfully!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    captcha_model_workflow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
